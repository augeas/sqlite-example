{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing with Spacy: Practice Challenge\n",
    "Apparently, Harry Potter [fan-fiction](https://en.wikipedia.org/wiki/Fan_fiction) is worthy of [serious](http://www.the-leaky-cauldron.org/features/essays/issue4/genretheory/) [academic](https://wesscholar.wesleyan.edu/cgi/viewcontent.cgi?article=2031&context=etd_hon_theses) [study](https://dspace.library.uu.nl/bitstream/handle/1874/315567/BAThesis%20Janieke%20Koning%203858863.pdf;sequence=1). We're going to build a machine that analyses Harry Potter fan-fiction, so no-one else has to read any if they don't want to. You'll need to install [spaCy](https://spacy.io/usage/), probably using [Anaconda](https://anaconda.org/conda-forge/spacy). You'll also need the [English model](https://spacy.io/usage/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from unittest import TestCase, TestLoader, TextTestRunner\n",
    "from itertools import  chain, count, takewhile, tee\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import spacy\n",
    "\n",
    "def runTest(case):\n",
    "    suite = TestLoader().loadTestsFromModule(case())\n",
    "    TextTestRunner().run(suite)   \n",
    "\n",
    "# Make sure you've installed the English model first.\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here are some functions for grabbing stories from [www.fanfiction.net](https://www.fanfiction.net):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chapter_urls(url):\n",
    "    chunks = url.split('/')\n",
    "    yield from ('/'.join(chunks[:5]+[str(c)]+[chunks[-1]]) for c in count(1))\n",
    "    \n",
    "def chapter_content(urls):\n",
    "        content = (requests.get(u).content for u in urls)\n",
    "        soups = (BeautifulSoup(c, \"html5lib\").find_all(\"div\", class_=\"storytext\") for c in content)\n",
    "        texts = chain.from_iterable(takewhile(lambda s: len(s) > 0, soups))\n",
    "        yield from map(lambda t: t.text, texts)\n",
    "        \n",
    "def get_fanfic(url):\n",
    "    return '\\n'.join(chapter_content(chapter_urls(url)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to load [this story](https://www.fanfiction.net/s/12616526/1/Fight-or-Flight) into spacy first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.fanfiction.net/s/12616526/1/Fight-or-Flight'\n",
    "fanfic = get_fanfic(url)\n",
    "nlp_fanfic = nlp(fanfic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(fanfic[0:256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Named Entity Recognition\n",
    "The extraction and classification of proper nouns is called named entity recognition. Look at the [types of entities](https://spacy.io/api/annotation#named-entities) that spacy's model has been trained to recognise. We'll use the following tags as possible locations in the story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "location_tags = ['NORP', 'FACILITY', 'ORG', 'LOC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document we generated from the story has an \"*ents*\" attribute that holds all the named entities. We'll collect all the entities whose \"*label_*\" attribute is in \"*location_tags*\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "location_entities = [ent for ent in nlp_fanfic.ents if ent.label_ in location_tags]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first 10. Not *too* bad. Apparrently \"Lorcan\" is a character. Your humble narrator had to look that up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "location_entities[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"root\" of a word is called it's *lemma*. Look at the \"*lemma_*\" attribute of the 5th entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "location_entities[4].lemma_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"*subtree*\" attribute of an entitity helps describe its context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree = list(location_entities[4].subtree)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging\n",
    "\n",
    "Each token in the tree has a \"*pos_*\" attribute that contains its part of speech. \"Gryffindor\" is correctly identified as a propper noun. \"Potter\" should be too, but it's been described as a regular noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(tree[1].pos_, tree[4].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the [\"collections.Counter\" class](https://docs.python.org/3/library/collections.html#collections.Counter) to get the frequencies of the location entities. We get a dictionary-like object whose keys are the lemmas, and whose values are the number of times each lemma appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "location_freqs = Counter([e.lemma_ for e in location_entities])\n",
    "location_freqs['gryffindor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.\n",
    "Your turn, complete the function \"*filter_freq_count*\", that removes all the lemmas that have a frequency less than the given threshold. You *might* want to use the built in [filter](https://docs.python.org/3/library/functions.html#filter) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_freq_count(freq, threshold=2):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.\n",
    "Complete the \"*sorted_freq_count*\" function. It should return a list of tuples, the first value is the lemma, the second is the frequency. The highests frequency should be returned first, you'll need the [sorted](https://docs.python.org/3/library/functions.html#sorted) built-in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sorted_freq_count(freq):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the top 10 locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_freq_count(location_freqs)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.\n",
    "Populate the list \"*character_entities*\" so that it contains all the entities from our document whose label is \"*Person*\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "character_entities = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4.\n",
    "Now complete the \"*character_lemmas*\" list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "character_lemmas = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've got everything right, this should give us the top 15 most frequently occuring characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_freq_count(Counter(character_lemmas))[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5.\n",
    "Given a list of entities and a lemma, the \"*lemma_filter*\" function should return only those entities that have that lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lemma_filter(lemma, ents):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part 6\n",
    "Complete the \"*entity_adjectives*\" function, which takes a sequence of entities. It should search for all the tokens in all the entities whose \"*pos_*\" attribute is \"ADJ\" for adjective. Return the sequence of lemmas for the adjectives with your \"*sorted_freq_count*\" function applied to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def entity_adjectives(entities):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's get the entities for some of the most frequently occuring characters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "roxanne = list(lemma_filter('roxanne', character_entities))\n",
    "james = list(lemma_filter('james', character_entities))\n",
    "violet = list(lemma_filter('violet', character_entities))\n",
    "fred = list(lemma_filter('fred', character_entities))\n",
    "lysander = list(lemma_filter('lysander', character_entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use your function to see how they're described. Some of the results are quite reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for char in ['roxanne', 'james', 'violet', 'fred', 'lysander']:\n",
    "    print('adjectives for '+char+': ', entity_adjectives(locals()[char])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
